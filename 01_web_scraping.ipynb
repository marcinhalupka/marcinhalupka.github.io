{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5242c195",
   "metadata": {},
   "source": [
    "# Web scraping in Python with Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f947ac",
   "metadata": {},
   "source": [
    "List of all topics that will be covered in this notebook:\n",
    "\n",
    "1. **Introduction to web scraping:**\n",
    "    * What is web scraping and why is it useful?\n",
    "    * Legal and ethical considerations when scraping data from websites\n",
    "    \n",
    "    \n",
    "2. **Getting started with Beautiful Soup:**\n",
    "    * Installing and importing Beautiful Soup library\n",
    "    * Overview of HTML and CSS syntax\n",
    "    \n",
    "    \n",
    "3. **Parsing HTML with Beautiful Soup:**\n",
    "    * Creating a BeautifulSoup object\n",
    "    * Navigating and searching through HTML tags and elements\n",
    "    * A bit more advanced techniques\n",
    "    \n",
    "    \n",
    "4. **Scraping data from websites:**\n",
    "    * Identifying target data on a website\n",
    "    * Writing a script to scrape data from a single page\n",
    "    * Iterating through multiple pages to scrape data\n",
    "    \n",
    "    \n",
    "5. **Handling common challenges:**\n",
    "    * Dealing with inconsistent HTML structure\n",
    "    * Handling pagination and dynamic content\n",
    "    \n",
    "    \n",
    "6. **Advanced techniques and use cases:**\n",
    "    * Scraping data from AJAX calls and APIs\n",
    "    * Extracting data from PDFs and other file types\n",
    "    * Building a web scraper to automate data collection\n",
    "    \n",
    "    \n",
    "7. **Best practices and ethical considerations:**\n",
    "    * Data cleaning and validation\n",
    "    * Caching and rate limiting to avoid overloading websites\n",
    "    \n",
    "    \n",
    "8. **Conclusion and next steps:**\n",
    "    * Recap of key concepts and techniques\n",
    "    * Resources for further learning and practice\n",
    "    * Discussion on potential use cases for web scraping in real-world projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e33f9c",
   "metadata": {},
   "source": [
    "# 1. Introduction to web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f7b7b5",
   "metadata": {},
   "source": [
    "1. What is web scraping and why is it useful?\n",
    "2. Legal and ethical considerations when scraping data from websites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a33fc4",
   "metadata": {},
   "source": [
    "## 1.1 What is web scraping and why is it useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42792070",
   "metadata": {},
   "source": [
    "Web scraping is the process of extracting data from websites using automated software. It involves analyzing the HTML structure of a website, identifying the data that you want to extract, and using code to extract that data. The extracted data can then be stored in a database, used for data analysis, or displayed on a website.\n",
    "\n",
    "Web scraping has become increasingly popular in recent years as more and more data is made available on the web. Many businesses and organizations use web scraping to gather data for market research, competitor analysis, and other purposes. Individual users may also use web scraping to collect data for personal projects or research.\n",
    "\n",
    "Web scraping can be incredibly useful for a variety of reasons. Here are a few examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd4d36",
   "metadata": {},
   "source": [
    "### Accessing data that isn't otherwise available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9717f907",
   "metadata": {},
   "source": [
    "Many websites provide valuable data that isn't available through APIs or other means of access. By scraping these websites, you can gain access to this data and use it for your own purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e460883c",
   "metadata": {},
   "source": [
    "### Automating data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a934df",
   "metadata": {},
   "source": [
    "If you need to collect data from a large number of websites or pages, web scraping can save you a lot of time and effort compared to manually copying and pasting data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c4a7c3",
   "metadata": {},
   "source": [
    "### Data analysis and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b13540",
   "metadata": {},
   "source": [
    "Once you've collected data through web scraping, you can analyze and visualize it to gain insights or tell a story. For example, you might scrape data from job listings to analyze trends in job titles or salary ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c315dd46",
   "metadata": {},
   "source": [
    "### Monitoring changes over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e816e6",
   "metadata": {},
   "source": [
    "By scraping a website at regular intervals, you can track changes over time. This can be useful for tracking prices of products, monitoring news or social media feeds, or monitoring changes to a website's structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7aee96",
   "metadata": {},
   "source": [
    "## 1.2 Legal and ethical considerations when scraping data from websites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b612c818",
   "metadata": {},
   "source": [
    "While web scraping can be a powerful tool for accessing and analyzing data from the web, it's important to be aware of the legal and ethical considerations involved. Here are some key things to keep in mind:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c25d6f4",
   "metadata": {},
   "source": [
    "### Check the website's terms of service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c88180",
   "metadata": {},
   "source": [
    "Before scraping data from a website, it's important to check the website's terms of service to see if there are any restrictions on scraping or use of their data. Some websites explicitly prohibit web scraping in their terms of service, while others may allow it with certain limitations. If you violate a website's terms of service, you may be subject to legal action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb90552",
   "metadata": {},
   "source": [
    "### Respect website owners' copyrights and intellectual property"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6e89f9",
   "metadata": {},
   "source": [
    "In addition to terms of service, it's important to respect website owners' copyrights and intellectual property rights. This means not using scraped data in ways that violate copyright law, such as reproducing or distributing copyrighted content without permission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0799f264",
   "metadata": {},
   "source": [
    "### Be mindful of website performance and bandwidth usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d226a25b",
   "metadata": {},
   "source": [
    "Scraping a website can put a strain on the website's servers and impact website performance for other users. It's important to be mindful of this and to avoid overloading a website with excessive requests. You can do this by setting reasonable request rates and using techniques like caching to minimize the number of requests you need to make."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2d12dd",
   "metadata": {},
   "source": [
    "### Don't scrape personal or sensitive data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e45ad8",
   "metadata": {},
   "source": [
    "Scraping personal or sensitive data from websites, such as login credentials, personal information, or financial data, can be illegal and unethical. It's important to only scrape data that is publicly available and not intended to be private."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d96b1a4",
   "metadata": {},
   "source": [
    "### Respect the privacy of website users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca35445",
   "metadata": {},
   "source": [
    "When scraping data from websites, it's important to respect the privacy of website users. This means not collecting data that could be used to identify individual users without their consent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804fbe21",
   "metadata": {},
   "source": [
    "### Be transparent about your scraping activities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b021b6",
   "metadata": {},
   "source": [
    "If you're scraping data from a website, it's important to be transparent about your activities. This means providing clear information about what data you're collecting, how you're using it, and who you're sharing it with (if anyone). Transparency can help build trust with website owners and users and minimize the risk of legal or ethical issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600cb309",
   "metadata": {},
   "source": [
    "# 2. Getting started with Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a85730",
   "metadata": {},
   "source": [
    "1. Installing and importing Beautiful Soup library\n",
    "2. Overview of HTML and CSS syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773d7dc3",
   "metadata": {},
   "source": [
    "## 2.1 Installing and importing Beautiful Soup library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6c0201",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python library that makes it easy to scrape data from HTML and XML files. Before you can use Beautiful Soup in your Python code, you'll need to install it and import it into your project. Here's how to do that:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc17fc0",
   "metadata": {},
   "source": [
    "### Installing Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa30180",
   "metadata": {},
   "source": [
    "To install Beautiful Soup, you can use pip, which is a package manager for Python. Open up your terminal or command prompt and run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ccdbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4377b4",
   "metadata": {},
   "source": [
    "This will download and install the latest version of Beautiful Soup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ef5361",
   "metadata": {},
   "source": [
    "### Importing Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4370aeaa",
   "metadata": {},
   "source": [
    "Once you've installed Beautiful Soup, you can import it into your Python code using the following statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "907debbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3c1d82",
   "metadata": {},
   "source": [
    "This imports the `BeautifulSoup` class from the `bs4` module, which is the main interface for using Beautiful Soup. You can now create instances of the `BeautifulSoup` class to parse HTML and XML files and extract data from them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74f61da",
   "metadata": {},
   "source": [
    "### Verifying installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b1aa68",
   "metadata": {},
   "source": [
    "To verify that Beautiful Soup has been installed correctly, you can open a Python interpreter or create a new Python script and enter the following code:To verify that Beautiful Soup has been installed correctly, you can open a Python interpreter or create a new Python script and enter the following code:\n",
    "\n",
    "This code creates a new instance of the `BeautifulSoup` class and passes in a simple HTML document as a string. The `prettify()` method is then called to print out the HTML with indentation to make it more readable.\n",
    "\n",
    "If everything is working correctly, you should see the HTML document printed out with indentation. This confirms that you've successfully installed and imported Beautiful Soup and can start using it to scrape data from HTML and XML files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a449b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   Test\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <p>\n",
      "   This is a test.\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(\"<html><head><title>Test</title></head><body><p>This is a test.</p></body></html>\")\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cf270c",
   "metadata": {},
   "source": [
    "This code creates a new instance of the `BeautifulSoup` class and passes in a simple HTML document as a string. The `prettify()` method is then called to print out the HTML with indentation to make it more readable.\n",
    "\n",
    "If everything is working correctly, you should see the HTML document printed out with indentation. This confirms that you've successfully installed and imported Beautiful Soup and can start using it to scrape data from HTML and XML files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbc1e59",
   "metadata": {},
   "source": [
    "## 2.2 Overview of HTML and CSS syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500c1f03",
   "metadata": {},
   "source": [
    "HTML (Hypertext Markup Language) and CSS (Cascading Style Sheets) are the building blocks of the web. HTML provides the structure and content of web pages, while CSS provides the styling and layout. As a web scraper, it's important to have a basic understanding of HTML and CSS syntax in order to effectively navigate and extract data from web pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b105b2a",
   "metadata": {},
   "source": [
    "### HTML Syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1c63b6",
   "metadata": {},
   "source": [
    "HTML is made up of tags, which define the structure and content of a web page. Tags are enclosed in angle brackets and usually come in pairs, with an opening tag and a closing tag. For example, the `<html>` tag is used to enclose the entire web page, and the `<head>` and `<body>` tags are used to separate the page's head and body sections.\n",
    "\n",
    "Tags can also have attributes, which provide additional information about the tag or modify its behavior. Attributes are included in the opening tag and are written as name-value pairs. For example, the `<img>` tag has a `src` attribute that specifies the location of the image to be displayed.\n",
    "\n",
    "Some elements of a web page consist of an opening tag, a closing tag, and the content in between. For example, the `<p>` element is used to enclose a paragraph of text, and the `<a>` element is used to create a hyperlink.\n",
    "\n",
    "HTML elements can also be self-closing, meaning they don't require a closing tag. For example, the `<img>` element is used to display an image and is self-closing, as it doesn't have any content between its tags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5494ab7b",
   "metadata": {},
   "source": [
    "### CSS Syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d18d51b",
   "metadata": {},
   "source": [
    "CSS is used to add styling and layout to web pages. CSS styles are defined in a separate file or within the HTML document using a `<style>` tag. CSS styles consist of a selector and a set of properties and values. The selector specifies which HTML elements the style should be applied to, and the properties and values define how the elements should be styled.\n",
    "\n",
    "For example, to change the color of all `<h1>` tags on a web page to red, you would use the following CSS code:\n",
    "\n",
    "```css\n",
    "h1 {\n",
    "  color: red;\n",
    "}\n",
    "```\n",
    "\n",
    "Here, `h1` is the selector, and `color: red` is the property-value pair that defines the style."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e7de3e",
   "metadata": {},
   "source": [
    "### Using HTML and CSS in Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f1283e",
   "metadata": {},
   "source": [
    "When scraping data from web pages, it's important to have a basic understanding of HTML and CSS syntax in order to effectively navigate the page and extract the desired data. By inspecting the HTML and CSS of a web page, you can identify the tags and attributes that contain the data you're interested in and use this information to write effective scraping code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd230e73",
   "metadata": {},
   "source": [
    "# 3. Parsing HTML with Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedcd4fb",
   "metadata": {},
   "source": [
    "1. Creating a BeautifulSoup object\n",
    "2. Navigating and searching through HTML tags and elements\n",
    "3. A bit more advanced techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f615d9d",
   "metadata": {},
   "source": [
    "## 3.1 Creating a BeautifulSoup object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a86bc5",
   "metadata": {},
   "source": [
    "To create a `BeautifulSoup` object, we need to provide it with the HTML content we want to parse. We can do this by passing the HTML content as a string to the `BeautifulSoup` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0f2d47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = '<html><body><h1>Hello, World!</h1></body></html>'\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8155f0",
   "metadata": {},
   "source": [
    "In this example, we've created a `BeautifulSoup` object called `soup` by passing it the HTML content as a string and specifying the parser to be used (`'html.parser'` in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef4d0e2",
   "metadata": {},
   "source": [
    "## 3.2 Navigating and searching through HTML tags and elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4205185",
   "metadata": {},
   "source": [
    "Now that we have a `BeautifulSoup` object, we can use its methods and attributes to navigate and search the parse tree for specific tags and elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52abfd87",
   "metadata": {},
   "source": [
    "### Accessing tags and attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdebda31",
   "metadata": {},
   "source": [
    "We can access tags and attributes of a BeautifulSoup object by calling the tag name as an attribute, or by using the find() method. For example, let's say we have the following HTML code:\n",
    "\n",
    "```html\n",
    "<html>\n",
    "  <head>\n",
    "    <title>My Web Page</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Welcome to my web page</h1>\n",
    "    <p>This is some paragraph text.</p>\n",
    "    <ul>\n",
    "      <li class=\"list-item\">Item 1</li>\n",
    "      <li class=\"list-item\">Item 2</li>\n",
    "      <li class=\"list-item\">Item 3</li>\n",
    "    </ul>\n",
    "  </body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "We can access the title tag and its contents like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e09ae19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Web Page\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = '''<html>\n",
    "  <head>\n",
    "    <title>My Web Page</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>Welcome to my web page</h1>\n",
    "    <p>This is some paragraph text.</p>\n",
    "    <ul>\n",
    "      <li class=\"list-item\">Item 1</li>\n",
    "      <li class=\"list-item\">Item 2</li>\n",
    "      <li class=\"list-item\">Item 3</li>\n",
    "    </ul>\n",
    "  </body>\n",
    "</html>'''\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "title_tag = soup.title\n",
    "title_text = title_tag.string\n",
    "\n",
    "print(title_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e901187",
   "metadata": {},
   "source": [
    "Alternatively, we can use the `find()` method to access the title tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93ac4da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Web Page\n"
     ]
    }
   ],
   "source": [
    "title_tag = soup.find('title')\n",
    "title_text = title_tag.string\n",
    "\n",
    "print(title_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c1436d",
   "metadata": {},
   "source": [
    "We can also access attributes of a tag using dictionary-style indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "349358c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['list-item']\n",
      "['list-item']\n",
      "['list-item']\n"
     ]
    }
   ],
   "source": [
    "ul_tag = soup.find('ul')\n",
    "li_tags = ul_tag.find_all('li')\n",
    "\n",
    "for li_tag in li_tags:\n",
    "    print(li_tag['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa7b420",
   "metadata": {},
   "source": [
    "In this example, we're accessing the class attribute of each li tag in the ul tag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a98b4c",
   "metadata": {},
   "source": [
    "### Searching for tags and elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44da20a",
   "metadata": {},
   "source": [
    "In addition to accessing specific tags and attributes, we can also search the parse tree for tags and elements that match certain criteria. The `find_all()` method is particularly useful for this.\n",
    "\n",
    "For example, let's say we want to find all the `li` tags in the `ul` tag. We can do this using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a161ff9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 1\n",
      "Item 2\n",
      "Item 3\n"
     ]
    }
   ],
   "source": [
    "ul_tag = soup.find('ul')\n",
    "li_tags = ul_tag.find_all('li')\n",
    "\n",
    "for li_tag in li_tags:\n",
    "    print(li_tag.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76f6054",
   "metadata": {},
   "source": [
    "This will print out the contents of each `li` tag.\n",
    "\n",
    "We can also search for tags and elements based on their attributes. For example, let's say we want to find all the links (`a` tags) that have the attribute `href` equal to `\"https://www.google.com\"`. We can do this using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8474b8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find_all('a', href='https://www.google.com')\n",
    "\n",
    "for link in links:\n",
    "    print(link.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5939e893",
   "metadata": {},
   "source": [
    "This will print out the contents of each link that matches the specified criteria (nothing in our case, as we don't have links matching the criteria in our example)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58971241",
   "metadata": {},
   "source": [
    "## 3.3 A bit more advanced techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2d807e",
   "metadata": {},
   "source": [
    "So far, we've covered some basic techniques for navigating and searching through HTML tags and elements using Beautiful Soup. In this chapter, we'll explore some more advanced techniques that can help you extract data more efficiently and effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545091c5",
   "metadata": {},
   "source": [
    "### Using CSS selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebf2fa9",
   "metadata": {},
   "source": [
    "In addition to using tag names and attributes to search for elements, Beautiful Soup also supports CSS selectors. CSS selectors are patterns that match against elements in an HTML document, based on their tag names, attributes, and other characteristics.\n",
    "\n",
    "For example, let's say we have the following HTML code:\n",
    "\n",
    "```html\n",
    "<div class=\"product\">\n",
    "  <h2 class=\"title\">Product 1</h2>\n",
    "  <p class=\"description\">This is a description of Product 1.</p>\n",
    "  <span class=\"price\">$10.00</span>\n",
    "</div>\n",
    "<div class=\"product\">\n",
    "  <h2 class=\"title\">Product 2</h2>\n",
    "  <p class=\"description\">This is a description of Product 2.</p>\n",
    "  <span class=\"price\">$15.00</span>\n",
    "</div>\n",
    "<div class=\"product\">\n",
    "  <h2 class=\"title\">Product 3</h2>\n",
    "  <p class=\"description\">This is a description of Product 3.</p>\n",
    "  <span class=\"price\">$20.00</span>\n",
    "</div>\n",
    "```\n",
    "\n",
    "We can use a CSS selector to find all the elements with class `product`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5155c8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''<div class=\"product\">\n",
    "            <h2 class=\"title\">Product 1</h2>\n",
    "            <p class=\"description\">This is a description of Product 1.</p>\n",
    "            <span class=\"price\">$10.00</span>\n",
    "          </div>\n",
    "          <div class=\"product\">\n",
    "            <h2 class=\"title\">Product 2</h2>\n",
    "            <p class=\"description\">This is a description of Product 2.</p>\n",
    "            <span class=\"price\">$15.00</span>\n",
    "          </div>\n",
    "          <div class=\"product\">\n",
    "            <h2 class=\"title\">Product 3</h2>\n",
    "            <p class=\"description\">This is a description of Product 3.</p>\n",
    "            <span class=\"price\">$20.00</span>\n",
    "          </div>'''\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b026515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product 1 This is a description of Product 1. $10.00\n",
      "Product 2 This is a description of Product 2. $15.00\n",
      "Product 3 This is a description of Product 3. $20.00\n"
     ]
    }
   ],
   "source": [
    "products = soup.select('.product')\n",
    "\n",
    "for product in products:\n",
    "    title = product.select_one('.title').text\n",
    "    description = product.select_one('.description').text\n",
    "    price = product.select_one('.price').text\n",
    "    print(title, description, price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f81021",
   "metadata": {},
   "source": [
    "This printed out the title, description, and price for each product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cb3429",
   "metadata": {},
   "source": [
    "### Navigating the parse tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9818af9",
   "metadata": {},
   "source": [
    "Beautiful Soup allows you to navigate the parse tree of an HTML document using a variety of methods and attributes. One useful attribute is parent, which allows you to access the parent element of an element in the parse tree.\n",
    "\n",
    "For example, let's look at that part of ou HTML code:\n",
    "\n",
    "```html\n",
    "<div class=\"product\">\n",
    "  <h2 class=\"title\">Product 1</h2>\n",
    "  <p class=\"description\">This is a description of Product 1.</p>\n",
    "  <span class=\"price\">$10.00</span>\n",
    "</div>\n",
    "```\n",
    "\n",
    "We can use the `parent` attribute to access the `div` element that contains this product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54ef0177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<div class=\"product\">\n",
      "<h2 class=\"title\">Product 1</h2>\n",
      "<p class=\"description\">This is a description of Product 1.</p>\n",
      "<span class=\"price\">$10.00</span>\n",
      "</div>\n"
     ]
    }
   ],
   "source": [
    "title_tag = soup.find('h2', {'class': 'title'})\n",
    "product_div = title_tag.parent\n",
    "\n",
    "print(product_div)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291861ed",
   "metadata": {},
   "source": [
    "This printed out the div element that contains the product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b93061",
   "metadata": {},
   "source": [
    "### Using regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0d34c",
   "metadata": {},
   "source": [
    "Beautiful Soup also supports regular expressions, which can be used to search for elements based on patterns in their tag names or attributes.\n",
    "\n",
    "For example, let's say make small changes to our previous HTML code:\n",
    "\n",
    "```html\n",
    "<div id=\"product-123\">\n",
    "  <h2 class=\"title\">Product 1</h2>\n",
    "  <p class=\"description\">This is a description of Product 1.</p>\n",
    "  <span class=\"price\">$10.00</span>\n",
    "</div>\n",
    "<div id=\"product-456\">\n",
    "  <h2 class=\"title\">Product 2</h2>\n",
    "  <p class=\"description\">This is a description of Product 2.</p>\n",
    "  <span class=\"price\">$15.00</span>\n",
    "</div>\n",
    "<div id=\"product-789\">\n",
    "  <h2 class=\"title\">Product 3</h2>\n",
    "  <p class=\"description\">This is a description of Product 3.</p>\n",
    "  <span class=\"price\">$20.00</span>\n",
    "</div>\n",
    "```\n",
    "\n",
    "We can use a regular expression to find all the `div` elements that have an `id` attribute starting with the string \"product-\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c773b30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = '''<div id=\"product-123\">\n",
    "            <h2 class=\"title\">Product 1</h2>\n",
    "            <p class=\"description\">This is a description of Product 1.</p>\n",
    "            <span class=\"price\">$10.00</span>\n",
    "          </div>\n",
    "          <div id=\"product-456\">\n",
    "            <h2 class=\"title\">Product 2</h2>\n",
    "            <p class=\"description\">This is a description of Product 2.</p>\n",
    "            <span class=\"price\">$15.00</span>\n",
    "          </div>\n",
    "          <div id=\"product-789\">\n",
    "            <h2 class=\"title\">Product 3</h2>\n",
    "            <p class=\"description\">This is a description of Product 3.</p>\n",
    "            <span class=\"price\">$20.00</span>\n",
    "          </div>'''\n",
    "\n",
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d395cbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product 1 This is a description of Product 1. $10.00\n",
      "Product 2 This is a description of Product 2. $15.00\n",
      "Product 3 This is a description of Product 3. $20.00\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "product_divs = soup.find_all('div', {'id': re.compile('^product-')})\n",
    "\n",
    "for product_div in product_divs:\n",
    "    title = product_div.find('h2', {'class': 'title'}).text\n",
    "    description = product_div.find('p', {'class': 'description'}).text\n",
    "    price = product_div.find('span', {'class': 'price'}).text\n",
    "    print(title, description, price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41421518",
   "metadata": {},
   "source": [
    "This printed out the title, description, and price for each product that matches the regular expression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eff8e0",
   "metadata": {},
   "source": [
    "### Handling errors and exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cef127f",
   "metadata": {},
   "source": [
    "When scraping data from websites, it's important to be aware of potential errors and exceptions that can occur. For example, a website might have changed its HTML structure, causing your code to break. Or the website might have implemented measures to prevent web scraping, such as rate limiting or blocking requests from known web scraping tools.\n",
    "\n",
    "To handle these types of situations, it's a good idea to include error handling and exception handling in your code. Here are some common techniques for handling errors and exceptions when web scraping:\n",
    "\n",
    "* **Try-except blocks:** Use a try-except block to catch and handle exceptions that might occur when scraping data from a website. For example, if a website is down or unavailable, you might want to handle the exception by retrying the request after a certain amount of time.\n",
    "\n",
    "* **Logging:** Use a logging module to keep track of any errors or exceptions that occur when scraping data from a website. This can help you identify and troubleshoot issues more easily.\n",
    "\n",
    "* **Rate limiting:** If a website has implemented measures to prevent web scraping, such as rate limiting or blocking requests from known web scraping tools, consider implementing rate limiting in your code to avoid being blocked or banned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e162c9",
   "metadata": {},
   "source": [
    "In this chapter, we've covered some more advanced techniques for navigating and searching through HTML tags and elements using Beautiful Soup. We've also discussed some strategies for handling errors and exceptions when scraping data from websites. With these techniques in your toolkit, you'll be well-equipped to extract data from a wide variety of websites using Beautiful Soup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae3fc1d",
   "metadata": {},
   "source": [
    "# 4. Scraping data from websites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada141c",
   "metadata": {},
   "source": [
    "1. Identifying target data on a website\n",
    "2. Writing a script to scrape data from a single page\n",
    "3. Iterating through multiple pages to scrape data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6c2158",
   "metadata": {},
   "source": [
    "## 4.1 Identifying target data on a website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e784c17",
   "metadata": {},
   "source": [
    "The first step in web scraping is to identify the data you want to extract from a website. This can be a straightforward process if the website has a simple and consistent HTML structure. However, if the website has a more complex structure or if the data is spread across multiple pages, it can be more challenging to identify the target data.\n",
    "\n",
    "Here are some tips for identifying target data on a website:\n",
    "\n",
    "* **Inspect the website's HTML:** Use your web browser's developer tools to inspect the website's HTML structure. This will give you an idea of the structure and hierarchy of the website's elements, and help you identify the elements that contain the data you want to scrape.\n",
    "\n",
    "* **Look for patterns and consistency:** If the website has a consistent HTML structure or if the data is presented in a consistent way, look for patterns that you can use to identify the target data. For example, the target data might be contained within elements with a specific class or ID.\n",
    "\n",
    "* **Consider the context:** When identifying target data on a website, it's important to consider the context in which the data appears. For example, if you're scraping product data from an e-commerce website, you'll want to make sure you're extracting the correct data for each product, and not accidentally scraping data from unrelated elements on the page.\n",
    "\n",
    "* **Use trial and error:** In some cases, the process of identifying target data on a website might require some trial and error. Try out different approaches and test your code on a small subset of the data before scaling up to scrape the entire website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e54e843",
   "metadata": {},
   "source": [
    "## 4.2 Writing a script to scrape data from a single page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab97200",
   "metadata": {},
   "source": [
    "Now that we've covered the basics of web scraping and how to use Beautiful Soup, let's dive into the process of actually scraping data from websites.\n",
    "\n",
    "Once you've identified the target data on a website, the next step is to write a Python script to scrape that data. In this section, we'll cover how to write a script to scrape data from a single web page.\n",
    "\n",
    "Here's an example script that demonstrates how to use Beautiful Soup to scrape data from a single page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd0ced1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the URL you want to scrape\n",
    "url = \"https://www.example.com\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using Beautiful Soup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find the element that contains the target data\n",
    "target_element = soup.find(\"p\")\n",
    "\n",
    "# Extract the target data from the element\n",
    "target_data = target_element.text.strip()\n",
    "\n",
    "# Print the scraped data\n",
    "print(target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e02c7a",
   "metadata": {},
   "source": [
    "**Let's break down this script step by step:**\n",
    "\n",
    "1. We start by importing the `requests` library and the `BeautifulSoup` class from the `bs4` module.\n",
    "\n",
    "2. Next, we send a GET request to the URL we want to scrape using the `requests.get()` function. This returns a `Response` object that contains the HTML content of the web page.\n",
    "\n",
    "3. We then create a `BeautifulSoup` object by passing the HTML content to the `BeautifulSoup()` constructor. This creates a parse tree that we can search and navigate using Beautiful Soup's methods and attributes.\n",
    "\n",
    "4. We use the `soup.find()` method to find the HTML element that contains the target data. In this example, we're looking for a `p` element.\n",
    "\n",
    "5. Once we've found the target element, we extract the target data from it using the `.text` attribute and the `.strip()` method to remove any spaces at the beginning and at the end of the string.\n",
    "\n",
    "6. Finally, we print the scraped data to the console.\n",
    "\n",
    "This is a simple example, but it demonstrates the basic process of scraping data from a single web page using Beautiful Soup. Of course, the specifics of your script will depend on the structure of the website you're scraping and the data you're trying to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe1c924",
   "metadata": {},
   "source": [
    "## 4.3 Iterating through multiple pages to scrape data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813001d9",
   "metadata": {},
   "source": [
    "Sometimes the data you need is spread across multiple pages of a website. In that case, you'll need to write a script that can iterate through each page and scrape the target data. Here's an example script that demonstrates how to use Beautiful Soup to scrape data from multiple pages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ae52670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1: ['Example Domain\\nThis domain is for use in illustrative examples in documents. You may use this\\n    domain in literature without prior coordination or asking for permission.\\nMore information...']\n",
      "Page 2: ['Example Domain\\nThis domain is for use in illustrative examples in documents. You may use this\\n    domain in literature without prior coordination or asking for permission.\\nMore information...']\n",
      "Page 3: ['Example Domain\\nThis domain is for use in illustrative examples in documents. You may use this\\n    domain in literature without prior coordination or asking for permission.\\nMore information...']\n",
      "Page 4: ['Example Domain\\nThis domain is for use in illustrative examples in documents. You may use this\\n    domain in literature without prior coordination or asking for permission.\\nMore information...']\n",
      "Page 5: ['Example Domain\\nThis domain is for use in illustrative examples in documents. You may use this\\n    domain in literature without prior coordination or asking for permission.\\nMore information...']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set the URL template and the range of page numbers to scrape\n",
    "url_template = \"https://www.example.com/page={}\"\n",
    "page_range = range(1, 6)\n",
    "\n",
    "# Loop through each page number and scrape the target data\n",
    "for page_num in page_range:\n",
    "    # Construct the URL for the current page\n",
    "    url = url_template.format(page_num)\n",
    "\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse the HTML content using Beautiful Soup\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Find all the elements that contain the target data\n",
    "    target_elements = soup.find_all(\"div\")\n",
    "\n",
    "    # Extract the target data from each element and append it to a list\n",
    "    target_data_list = []\n",
    "    for element in target_elements:\n",
    "        target_data_list.append(element.text.strip())\n",
    "\n",
    "    # Print the scraped data for the current page\n",
    "    print(\"Page {}: {}\".format(page_num, target_data_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c414ff3",
   "metadata": {},
   "source": [
    "This example demonstrates how to use Beautiful Soup to scrape data from multiple pages of a website. It prints the same text for each page as the website we're trying to access has no different pages than the homepage and in every iteration we're scraping the same content.\n",
    "\n",
    "In some future cases you will be filling an URL template with different values (like in the snippet above), in other you will be going through a list of different URLs and sometimes you'll be forced to do something more creative - the specifics of your script will depend on the structure of the website you're scraping and the data you're trying to extract."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa51335",
   "metadata": {},
   "source": [
    "# 5. Handling common challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c087d87e",
   "metadata": {},
   "source": [
    "1. Dealing with inconsistent HTML structure\n",
    "2. Handling pagination and dynamic content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04b93b7",
   "metadata": {},
   "source": [
    "## 5.1 Dealing with incosistent HTML structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55fd6f4",
   "metadata": {},
   "source": [
    "One common challenge when web scraping is dealing with inconsistent HTML structure across different pages on the same website or across different websites. Inconsistent structure can make it difficult to write a scraper that works reliably and consistently.\n",
    "\n",
    "Fortunately, there are some techniques you can use to deal with this challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16332d14",
   "metadata": {},
   "source": [
    "### Use the `find_all()` method with a list of tag names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f735b30b",
   "metadata": {},
   "source": [
    "One way to deal with inconsistent HTML structure is to use the `find_all()` method with a list of tag names instead of a single tag name. For example, if you're scraping a website that uses different tag names for headings on different pages, you could use the following code to extract all headings:\n",
    "\n",
    "```python\n",
    "soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "```\n",
    "\n",
    "This will return a list of all `h1`, `h2`, `h3`, `h4`, `h5`, and `h6` tags on the page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed4d2a",
   "metadata": {},
   "source": [
    "### Use the `get()` method to access attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5f96bc",
   "metadata": {},
   "source": [
    "Another common issue with inconsistent HTML structure is when different pages or websites use different attribute names to store the same type of data. For example, one page might use `class=\"price\"` to store the price of a product, while another page might use `class=\"product-price\"`.\n",
    "\n",
    "To deal with this, you can use the `get()` method to access attributes by name instead of using the dot notation. For example, instead of using `tag.price` to access the price of a product, you could use `tag.get('price')`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bef98ad",
   "metadata": {},
   "source": [
    "### Use regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529696a6",
   "metadata": {},
   "source": [
    "Regular expressions can be a powerful tool for dealing with inconsistent HTML structure. For example, if you're scraping a website that uses different tag names for headings on different pages, you could use a regular expression to extract all headings:\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "heading_pattern = re.compile(r'h[1-6]')\n",
    "headings = soup.find_all(heading_pattern)\n",
    "```\n",
    "\n",
    "This will return a list of all `h1`, `h2`, `h3`, `h4`, `h5`, and `h6` tags on the page, regardless of the specific tag name used.\n",
    "\n",
    "However, be careful when using regular expressions for web scraping, as they can be brittle and prone to breaking if the HTML structure changes even slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea90d13",
   "metadata": {},
   "source": [
    "## 5.2 Handling pagination and dynamic content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88570a5",
   "metadata": {},
   "source": [
    "When scraping data from websites, you may encounter pages with pagination or dynamic content that loads as the user scrolls down the page. Here are some techniques to handle these common challenges:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b21bcee",
   "metadata": {},
   "source": [
    "### Inspect the page source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92ba772",
   "metadata": {},
   "source": [
    "First, inspect the page source to determine how pagination or dynamic content is implemented on the page. If pagination is implemented using a simple query string parameter, you can simply iterate over the different parameter values to scrape all the pages. If dynamic content is loaded using JavaScript, you may need to use a headless browser like Selenium to scrape the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91045ce4",
   "metadata": {},
   "source": [
    "### Use a library like requests-html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0e4ad1",
   "metadata": {},
   "source": [
    "If you're dealing with dynamic content, you can use a library like requests-html to render the page in a headless browser and then scrape the data. For example, to scrape a page with dynamic content using requests-html, you can do the following:\n",
    "\n",
    "```python\n",
    "from requests_html import HTMLSession\n",
    "\n",
    "session = HTMLSession()\n",
    "r = session.get(url)\n",
    "r.html.render()\n",
    "```\n",
    "\n",
    "This will render the page in a headless browser and allow any dynamic content to load."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d8e609",
   "metadata": {},
   "source": [
    "### Use pagination parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b5382",
   "metadata": {},
   "source": [
    "If pagination is implemented using query string parameters, you can use a loop to iterate over the different parameter values and scrape all the pages. For example, if the pagination parameter is page, you could use the following code:\n",
    "\n",
    "```python\n",
    "for i in range(1, num_pages + 1):\n",
    "    url = base_url + '?page=' + str(i)\n",
    "    # scrape data from page\n",
    "```\n",
    "\n",
    "This will scrape data from all pages from 1 to `num_pages`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739db8b0",
   "metadata": {},
   "source": [
    "### Use Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff259f7",
   "metadata": {},
   "source": [
    "If the website uses JavaScript to load content dynamically, you may need to use a browser automation tool like Selenium to scrape the data. Selenium can automate actions on a website like clicking buttons, filling out forms, and scrolling down the page to load more content.\n",
    "\n",
    "To use Selenium in Python, you'll need to install the `selenium` package and a driver for the specific browser you want to automate. Here's an example code snippet to get started:\n",
    "\n",
    "```python\n",
    "from selenium import webdriver\n",
    "\n",
    "# specify the path to the driver executable\n",
    "driver_path = 'path/to/driver'\n",
    "\n",
    "# create a new instance of the browser\n",
    "driver = webdriver.Chrome(executable_path=driver_path)\n",
    "\n",
    "# navigate to the website\n",
    "driver.get(url)\n",
    "\n",
    "# perform actions on the website, e.g. click a button or scroll down the page\n",
    "\n",
    "# scrape data from the page\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c9ac53",
   "metadata": {},
   "source": [
    "### Use an API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec0031d",
   "metadata": {},
   "source": [
    "Finally, if the website provides an API to access the data, you can use the API to retrieve the data directly instead of scraping the website. Check the website's documentation or contact the website owner to see if an API is available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8e0108",
   "metadata": {},
   "source": [
    "# 6. Advanced techniques and use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dbfef6",
   "metadata": {},
   "source": [
    "1. Scraping data from AJAX calls and APIs\n",
    "2. Extracting data from PDFs and other file types\n",
    "3. Building a web scraper to automate data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aeca6b",
   "metadata": {},
   "source": [
    "## 6.1 Scraping data from AJAX calls and APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88c13f9",
   "metadata": {},
   "source": [
    "Web applications that heavily rely on JavaScript may use AJAX (Asynchronous JavaScript and XML) to dynamically load data. AJAX allows web pages to update asynchronously by exchanging small amounts of data with the server, rather than reloading the entire page.\n",
    "\n",
    "When scraping data from a website that uses AJAX, it's important to know how to identify the endpoints that return data to the application. These endpoints are often referred to as APIs, even though they may not adhere to the typical RESTful API design pattern.\n",
    "\n",
    "To scrape data from an API, you can use the requests library in Python to send HTTP requests and receive responses. The response will typically be in JSON format, which can be easily parsed using Python's built-in JSON library.\n",
    "\n",
    "Here's an example code snippet that demonstrates how to scrape data from an API using the requests and json libraries:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://example.com/api/data\"\n",
    "response = requests.get(url)\n",
    "data = json.loads(response.text)\n",
    "\n",
    "# Extract the desired data from the JSON response\n",
    "for item in data['items']:\n",
    "    print(item['name'], item['description'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040a73c2",
   "metadata": {},
   "source": [
    "In this example, we're sending a GET request to the URL that returns the data we're interested in. The response is in JSON format, so we use the `json.loads()` function to parse it into a Python dictionary. We can then extract the desired data from the dictionary and process it as needed.\n",
    "\n",
    "When you try to execute that code, you'll probably see an error, as there's no API behind that URL. Treat that example as a code snippet that presents and idea and can be adapted to the individual case.\n",
    "\n",
    "It's important to note that not all APIs are publicly accessible, and some may require authentication or authorization before they can be accessed. Additionally, scraping data from an API may be subject to rate limits or other usage restrictions. It's always good to check the API documentation and terms of service before scraping data from it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be79065e",
   "metadata": {},
   "source": [
    "## 6.2 Extracting data from PDFs and other file types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36da211c",
   "metadata": {},
   "source": [
    "In addition to HTML, some websites may provide data in other file formats such as `PDF`, `CSV`, or `Excel`. Extracting data from these file types requires different techniques and tools than scraping data from HTML pages.\n",
    "\n",
    "For example, to extract data from a PDF file, you can use the `PyPDF2` library in Python. PyPDF2 allows you to extract text and metadata from PDF files, as well as merge and split PDF files.\n",
    "\n",
    "Here's an example code snippet that demonstrates how to extract text from a PDF file using `PyPDF2`:\n",
    "\n",
    "```python\n",
    "import PyPDF2\n",
    "\n",
    "pdf_file = open('example.pdf', 'rb')\n",
    "pdf_reader = PyPDF2.PdfFileReader(pdf_file)\n",
    "\n",
    "for page in range(pdf_reader.numPages):\n",
    "    page_obj = pdf_reader.getPage(page)\n",
    "    text = page_obj.extractText()\n",
    "    print(text)\n",
    "    \n",
    "pdf_file.close()\n",
    "```\n",
    "\n",
    "In this example, we're opening a PDF file and creating a `PdfFileReader` object. We then iterate over each page in the PDF file and extract the text using the `extractText()` method. The extracted text can then be processed and analyzed as needed.\n",
    "\n",
    "It's important to note that not all PDF files are created equal and the text extraction quality can vary depending on the file format and the tools used to create the PDF. Additionally, some PDF files may be password-protected or have other restrictions that prevent text extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6416bbdb",
   "metadata": {},
   "source": [
    "## 6.3 Building a web scraper to automate data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f35e552",
   "metadata": {},
   "source": [
    "Automating data collection with a web scraper can save time and increase efficiency, especially when dealing with large amounts of data from multiple sources. In this section, we'll cover the steps involved in building a web scraper to automate data collection using Beautiful Soup.\n",
    "\n",
    "1. **Identify the target website and data to be scraped:** The first step in building a web scraper is to identify the website and the data to be scraped. Determine the structure of the website and the location of the data to be scraped.\n",
    "\n",
    "2. **Inspect the HTML structure of the website:** Use the web browser's developer tools to inspect the HTML structure of the website and identify the relevant HTML tags and attributes that contain the data to be scraped.\n",
    "\n",
    "3. **Write a script to scrape the data:** Use Beautiful Soup to write a Python script that scrapes the data from the target website. The script should include functions to navigate and search through the HTML structure, extract the desired data, and store the data in a structured format such as a CSV file or a database.\n",
    "\n",
    "4. **Test the script:** Test the script on a small sample of data to ensure that it is working correctly and producing the expected output.\n",
    "\n",
    "5. **Schedule the script to run automatically:** Once the script is working correctly, schedule it to run automatically at regular intervals using a task scheduler or a cron job.\n",
    "\n",
    "6. **Monitor the results:** Monitor the results of the web scraper to ensure that it is running correctly and producing the expected output. Make any necessary adjustments to the script or the schedule as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294d9185",
   "metadata": {},
   "source": [
    "# 7. Best practices and ethical considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8025ffa",
   "metadata": {},
   "source": [
    "1. Data cleaning and validation\n",
    "2. Caching and rate limiting to avoid overloading websites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7595ff9b",
   "metadata": {},
   "source": [
    "## 7.1 Data cleaning and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a018d",
   "metadata": {},
   "source": [
    "Data cleaning and validation are important steps in the web scraping process. The data you collect may contain errors, inconsistencies, or missing values, which can affect the quality of your analysis. To ensure that your data is accurate and reliable, you should perform data cleaning and validation before analyzing it.\n",
    "\n",
    "Data cleaning involves identifying and correcting errors and inconsistencies in the data. For example, you may need to correct misspellings, remove duplicates, or standardize data formats. Data validation involves checking the accuracy and completeness of the data. For example, you may need to check if the data is within a reasonable range, if it matches your expectations, or if it contains any missing values.\n",
    "\n",
    "Here are some tips for data cleaning and validation in web scraping:\n",
    "\n",
    "1. **Check for duplicates:** Duplicates can occur if the same data is present in multiple pages or if the scraper retrieves the same data multiple times. \n",
    "\n",
    "2. **Standardize data formats:** Data from different sources may use different formats, such as date formats or units of measurement. Standardizing data formats can help you compare and analyze data easily.\n",
    "\n",
    "3. **Handle missing values:** Missing data can occur if the scraper is unable to retrieve some data or if the data is not present in the HTML.\n",
    "\n",
    "4. **Validate data:** Check if the data is within the expected range and if it matches your expectations. For example, if you are scraping product prices, check if the prices are within the expected range.\n",
    "\n",
    "5. **Save a copy of the raw data:** Always save a copy of the raw data before cleaning and validating it. This can help you troubleshoot any issues and ensure that you have a backup of the original data.\n",
    "\n",
    "Use Python libraries like Pandas to speed up that process. By following these tips, you can ensure that your scraped data is accurate, reliable, and ready for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aefd401",
   "metadata": {},
   "source": [
    "## 7.2 Caching and rate limiting to avoid overloading websites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca005f7",
   "metadata": {},
   "source": [
    "When building a web scraper, it's important to be aware of the impact it can have on the website being scraped. Sending too many requests too quickly can cause the website to slow down or even crash. In practice quite often there are mechanisms that prevent the website from that, however the probable outcome of such actions is us getting blacklisted.\n",
    "\n",
    "One way to avoid overloading a website is by implementing caching and rate limiting in your web scraper. Caching involves saving the results of previous requests and reusing them instead of making the same request again. This can significantly reduce the number of requests made to the website and improve the performance of the scraper.\n",
    "\n",
    "Rate limiting, on the other hand, involves limiting the number of requests made to a website within a certain period of time. This helps ensure that the website is not overwhelmed with requests and remains functional for other users.\n",
    "\n",
    "There are various tools and libraries available to implement caching and rate limiting in your web scraper, such as requests-cache and ratelimit. You can even go for much simpler, basic solution - using sleep function from time library. When implementing these techniques, it's important to balance the need for data with respect for the website and its resources.\n",
    "\n",
    "Additionally, it's important to be aware of any legal or ethical considerations when scraping data from websites. Some websites may explicitly prohibit scraping in their terms of service, while others may have implicit expectations for behavior. Websites can use a robots.txt file to indicate which parts of the site can be crawled by bots and which can't. Make sure to respect the instructions provided in the robots.txt file when scraping data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e6eba3",
   "metadata": {},
   "source": [
    "# 8. Conclusion and next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37007d49",
   "metadata": {},
   "source": [
    "1. Recap of key concepts and techniques\n",
    "2. Resources for further learning and practice\n",
    "3. Discussion on potential use cases for web scraping in real-world projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07dc640",
   "metadata": {},
   "source": [
    "## 8.1 Recap of key concepts and techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b53064",
   "metadata": {},
   "source": [
    "Throughout this workshop, you have learned the fundamental concepts and techniques of web scraping using the Beautiful Soup library in Python. Here is a recap of some of the key takeaways:\n",
    "\n",
    "* Web scraping is the process of extracting data from websites using automated tools.\n",
    "* Beautiful Soup is a Python library that makes it easy to parse HTML and XML documents and extract the information you need.\n",
    "* HTML is the standard markup language used to create web pages, while CSS is used to style them.\n",
    "* Understanding the structure of HTML documents is essential for navigating and extracting data from web pages.\n",
    "* Beautiful Soup provides a range of powerful tools for navigating and searching through HTML documents, including tags, attributes, and regular expressions.\n",
    "* To scrape data from multiple pages, you can use a variety of techniques such as looping over URLs or using pagination.\n",
    "* When dealing with inconsistent HTML structure or dynamic content, you can use advanced techniques like parsing AJAX calls or using headless browsers.\n",
    "* Best practices for ethical web scraping include respecting website terms of service, using rate limiting, caching, and respecting robots.txt files.\n",
    "* Finally, data cleaning and validation are essential steps in any web scraping project to ensure the accuracy and reliability of your data.\n",
    "\n",
    "Now that you have a solid foundation in web scraping, you can continue to explore and expand your knowledge by experimenting with different websites, using other Python libraries, and exploring advanced use cases with use of such techniques as machine learning and natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d4321",
   "metadata": {},
   "source": [
    "## 8.2 Resources for further learning and practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e094b5",
   "metadata": {},
   "source": [
    "* **Beautiful Soup official documentation ([click](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)):** The official documentation is a great resource for learning more about the library and its various features. It includes a user guide, API reference, and many helpful examples.\n",
    "\n",
    "* **Web Scraping with Python book ([click](https://www.oreilly.com/library/view/web-scraping-with/9781491985564/)):** This book by Ryan Mitchell provides a comprehensive guide to web scraping with Python, including in-depth coverage of Beautiful Soup and other popular libraries.\n",
    "\n",
    "* **Dataquest's Web Scraping Tutorial ([click](https://www.dataquest.io/course/apis-and-scraping/)):** Dataquest offers a free web scraping tutorial that covers the basics of web scraping with Beautiful Soup, as well as more advanced techniques like pagination and handling dynamic content.\n",
    "\n",
    "* **Kaggle datasets ([click](https://www.kaggle.com/)):** Kaggle is a popular platform for data science and machine learning projects, and it includes a vast repository of datasets that can be used for practicing web scraping. Many of the datasets include web scraping examples and tutorials.\n",
    "\n",
    "* **Python requests library ([click](https://requests.readthedocs.io/en/latest/)):** The requests library is a popular Python library for making HTTP requests, which is often used in conjunction with Beautiful Soup for web scraping. Learning how to use requests effectively can be a valuable skill for web scraping.\n",
    "\n",
    "* **Scrapy framework ([click](https://scrapy.org/)):** Scrapy is a Python framework for web scraping that offers a more advanced and powerful set of tools than Beautiful Soup. Learning Scrapy can be a good next step for those looking to take their web scraping skills to the next level.\n",
    "\n",
    "* **Online communities ([click](https://www.reddit.com/r/webscraping/)):** Joining online communities like Reddit's /r/webscraping or Stack Overflow can be a great way to connect with other web scraping enthusiasts and get help with specific questions or challenges.\n",
    "\n",
    "* **Practice websites ([quotes](https://quotes.toscrape.com/),  [books](https://books.toscrape.com/)):** There are many websites designed specifically for practicing web scraping, such as ScrapingHub's \"Quotes to Scrape\" website or the \"Books to Scrape\" website created by Data Mining Club. These can be a good way to practice web scraping in a safe and controlled environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a314b1",
   "metadata": {},
   "source": [
    "## 8.3 Discussion on potential use cases for web scraping in real-world projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8da237",
   "metadata": {},
   "source": [
    "During this workshop, we've covered the basics of web scraping using Python's Beautiful Soup library. We've learned how to navigate and extract data from HTML, write scripts to scrape data from websites, handle common challenges, and build a web scraper to automate data collection.\n",
    "\n",
    "Now, let's discuss some potential use cases for web scraping in real-world projects. Web scraping can be used in various industries and scenarios, including:\n",
    "\n",
    "1. **Market research:** Scraping competitor websites to gather information on their pricing, product offerings, and marketing strategies.\n",
    "\n",
    "2. **Lead generation:** Scraping business directories, social media platforms, and job boards to identify potential clients or job candidates.\n",
    "\n",
    "3. **Data journalism:** Scraping public data sources to find trends and stories that can be used for reporting.\n",
    "\n",
    "4. **E-commerce:** Scraping product information from e-commerce websites to build a product catalog or monitor competitors' prices.\n",
    "\n",
    "5. **Finance:** Scraping financial news and data sources to make investment decisions or create predictive models.\n",
    "\n",
    "These are just a few examples, but the possibilities are endless. Keep practicing, experimenting, and innovating, and who knows what kind of amazing things you'll be able to create with web scraping!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254e2a90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
