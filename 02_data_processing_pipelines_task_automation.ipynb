{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd4aa967",
   "metadata": {},
   "source": [
    "# Data engineering - building pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46c41d4",
   "metadata": {},
   "source": [
    "There are several libraries that you can use to build a pipeline in Python. Some popular choices include:\n",
    "\n",
    "1. **Apache Beam** - An open-source library developed by Google that provides a simple, powerful programming model for building data processing pipelines. It supports both batch and streaming data processing, and can run on a variety of execution engines, including Apache Flink, Apache Spark, and Google Cloud Dataflow.\n",
    "\n",
    "2. **Luigi** - A Python library developed by Spotify for building complex pipelines of batch jobs. It helps you to define and orchestrate tasks, and to create dependencies between them, so that they can be run in the correct order.\n",
    "\n",
    "3. **Airflow** - An open-source platform developed by Airbnb for scheduling and managing data pipelines. It provides a simple interface for defining and organizing tasks, as well as monitoring and debugging them.\n",
    "\n",
    "4. **PySpark** - The Python API for Apache Spark, which is a fast and general-purpose cluster computing system. It provides a high-level API for distributed data processing, including operations for transforming, filtering, and aggregating data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a677913",
   "metadata": {},
   "source": [
    "# Apache Beam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2150c895",
   "metadata": {},
   "source": [
    "Apache Beam: An open-source library developed by Google that provides a simple, powerful programming model for building data processing pipelines. It supports both batch and streaming data processing, and can run on a variety of execution engines, including Apache Flink, Apache Spark, and Google Cloud Dataflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa8f135",
   "metadata": {},
   "source": [
    "A simple example of a data processing pipeline using Apache Beam in Python:\n",
    "\n",
    "```python\n",
    "import apache_beam as beam\n",
    "\n",
    "def process_data(data):\n",
    "    # Do some processing on the data here\n",
    "    return processed_data\n",
    "\n",
    "def run_pipeline():\n",
    "    # Create a Pipeline object\n",
    "    with beam.Pipeline() as pipeline:\n",
    "        # Read data from an input source (e.g. a file or a database)\n",
    "        data = pipeline | 'Read Data' >> beam.io.ReadFromSource()\n",
    "        \n",
    "        # Apply a processing function to the data\n",
    "        processed_data = data | 'Process Data' >> beam.Map(process_data)\n",
    "        \n",
    "        # Write the processed data to an output sink (e.g. a file or a database)\n",
    "        processed_data | 'Write Data' >> beam.io.WriteToSink()\n",
    "\n",
    "# Run the pipeline\n",
    "run_pipeline()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8d5d01",
   "metadata": {},
   "source": [
    "And another example:\n",
    "\n",
    "```python\n",
    "import apache_beam as beam\n",
    "\n",
    "# Create a Pipeline object\n",
    "pipeline = beam.Pipeline()\n",
    "\n",
    "# Define a data source, for example a text file\n",
    "lines = pipeline | 'ReadText' >> beam.io.ReadFromText('input.txt')\n",
    "\n",
    "# Define a data processing operation, for example a word count\n",
    "word_counts = (lines\n",
    "               | 'SplitWords' >> (beam.FlatMap(lambda x: x.split(' '))\n",
    "                                 .with_output_types(unicode))\n",
    "               | 'PairWithOne' >> beam.Map(lambda x: (x, 1))\n",
    "               | 'GroupAndSum' >> beam.CombinePerKey(sum))\n",
    "\n",
    "# Define a data sink, for example a text file\n",
    "word_counts | 'WriteText' >> beam.io.WriteToText('output.txt')\n",
    "\n",
    "# Run the pipeline\n",
    "pipeline.run()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6352c80",
   "metadata": {},
   "source": [
    "This example reads a text file as input, performs a word count on the contents, and writes the results to another text file. The pipeline is defined using the Pipeline object and a series of data transformation operations (e.g. FlatMap, Map, CombinePerKey) that are applied to the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce6461f",
   "metadata": {},
   "source": [
    "# Luigi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5ae763",
   "metadata": {},
   "source": [
    "Luigi: A Python library developed by Spotify for building complex pipelines of batch jobs. It helps you to define and orchestrate tasks, and to create dependencies between them, so that they can be run in the correct order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f40bd73",
   "metadata": {},
   "source": [
    "```python\n",
    "import luigi\n",
    "\n",
    "class ReadText(luigi.Task):\n",
    "    def requires(self):\n",
    "        # This task has no dependencies\n",
    "        return []\n",
    "    \n",
    "    def output(self):\n",
    "        # The output of this task is a text file\n",
    "        return luigi.LocalTarget('input.txt')\n",
    "    \n",
    "    def run(self):\n",
    "        # This task simply writes some text to the output file\n",
    "        with self.output().open('w') as out_file:\n",
    "            out_file.write('Hello World!\\n')\n",
    "\n",
    "class WordCount(luigi.Task):\n",
    "    def requires(self):\n",
    "        # This task depends on the ReadText task\n",
    "        return [ReadText()]\n",
    "    \n",
    "    def output(self):\n",
    "        # The output of this task is a text file\n",
    "        return luigi.LocalTarget('output.txt')\n",
    "    \n",
    "    def run(self):\n",
    "        # This task reads the input file, performs a word count, and writes the results to the output file\n",
    "        with self.input()[0].open() as in_file, self.output().open('w') as out_file:\n",
    "            counts = {}\n",
    "            for line in in_file:\n",
    "                for word in line.split(' '):\n",
    "                    if word in counts:\n",
    "                        counts[word] += 1\n",
    "                    else:\n",
    "                        counts[word] = 1\n",
    "            for word, count in counts.items():\n",
    "                out_file.write(f'{word}: {count}\\n')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    luigi.run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aded568",
   "metadata": {},
   "source": [
    "This example defines two tasks: ReadText and WordCount. The ReadText task generates a text file as output, and the WordCount task depends on this file as input. When the pipeline is run, Luigi will automatically ensure that the tasks are executed in the correct order, based on their dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19f5665",
   "metadata": {},
   "source": [
    "# Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5123c674",
   "metadata": {},
   "source": [
    "Airflow: An open-source platform developed by Airbnb for scheduling and managing data pipelines. It provides a simple interface for defining and organizing tasks, as well as monitoring and debugging them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535d81f7",
   "metadata": {},
   "source": [
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "def read_text():\n",
    "    # This function reads a text file and returns the contents\n",
    "    with open('input.txt', 'r') as f:\n",
    "        return f.read()\n",
    "\n",
    "def word_count(text):\n",
    "    # This function performs a word count on the input text and returns the results\n",
    "    counts = {}\n",
    "    for line in text.split('\\n'):\n",
    "        for word in line.split(' '):\n",
    "            if word in counts:\n",
    "                counts[word] += 1\n",
    "            else:\n",
    "                counts[word] = 1\n",
    "    return counts\n",
    "\n",
    "def write_counts(counts):\n",
    "    # This function writes the word counts to a text file\n",
    "    with open('output.txt', 'w') as f:\n",
    "        for word, count in counts.items():\n",
    "            f.write(f'{word}: {count}\\n')\n",
    "\n",
    "# Define a default_args dictionary to specify the parameters of the DAG\n",
    "default_args = {\n",
    "    'owner': 'me',\n",
    "    'start_date': days_ago(2),\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "# Create a DAG instance\n",
    "dag = DAG(\n",
    "    'my_pipeline',\n",
    "    default_args=default_args,\n",
    "    schedule_interval=timedelta(days=1),\n",
    ")\n",
    "\n",
    "# Define the tasks in the pipeline using PythonOperator\n",
    "read_task = PythonOperator(\n",
    "    task_id='read_text',\n",
    "    python_callable=read_text,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "count_task = PythonOperator(\n",
    "    task_id='word_count',\n",
    "    python_callable=word_count,\n",
    "    op_args=[read_task],\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "write_task = PythonOperator(\n",
    "    task_id='write_counts',\n",
    "    python_callable=write_counts,\n",
    "    op_args=[count_task],\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Set the dependencies between the tasks\n",
    "read_task >> count_task >> write_task\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4508ce",
   "metadata": {},
   "source": [
    "This example defines three tasks: read_text, word_count, and write_counts. The tasks are defined using the PythonOperator class, and their dependencies are specified using the >> operator. When the pipeline is run, Airflow will automatically ensure that the tasks are executed in the correct order, based on their dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6487b1b0",
   "metadata": {},
   "source": [
    "# PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17e7658",
   "metadata": {},
   "source": [
    "PySpark: The Python API for Apache Spark, which is a fast and general-purpose cluster computing system. It provides a high-level API for distributed data processing, including operations for transforming, filtering, and aggregating data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd699b0d",
   "metadata": {},
   "source": [
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.appName('my_pipeline').getOrCreate()\n",
    "\n",
    "# Read a text file as an RDD\n",
    "lines = spark.sparkContext.textFile('input.txt')\n",
    "\n",
    "# Perform a word count on the RDD\n",
    "word_counts = (lines\n",
    "               .flatMap(lambda x: x.split(' '))\n",
    "               .map(lambda x: (x, 1))\n",
    "               .reduceByKey(lambda x, y: x + y))\n",
    "\n",
    "# Write the word counts to a text file\n",
    "word_counts.saveAsTextFile('output.txt')\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f08f0ce",
   "metadata": {},
   "source": [
    "This example reads a text file as an RDD (Resilient Distributed Dataset), performs a word count on the contents, and writes the results to another text file. The pipeline is defined using a series of transformations (e.g. flatMap, map, reduceByKey) that are applied to the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5571494c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
